---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a senior undergraduate student majoring in Artificial Intelligence at Northeastern University, China. Currently, I have been working as a Research Assistant working on **Embodied AI, Spatial Intelligence and Robotics** under the supervision of [Prof. Mingyu Ding](https://dingmyu.github.io) at UNC-Chapel Hill and [Prof. Yao (Mark) Mu](https://yaomarkmu.github.io/) at [ScaleLab, SJTU](https://scalelab-sjtu.github.io/index.html). 


My research interests mainly focus on:
- **Grounding language in spatial understanding and robotic manipulation**
- **Egocentric embodied self-perception of MLLMs**
- **Reasoning and planning in multi-modality and embodied AI systems**
- **Human-Robot Collaborative Interaction in Real-Simulation Hybrid Environments**

My long-term research goal is to develop embodied agents that learn from rich, physically-informed data gathered through real-world interaction. I am dedicated to imbuing these agents with the capacity for causal reasoning and sophisticated decision-making, enabling them to collaborate seamlessly and safely with humans. This pursuit is guided by a core principle: to create technology **with everyone**, **for everyone**, ensuring that advancements in AI provide **accessible assistance** to people from all walks of life.

<span style="color:blue; font-weight:bold">‚ú® I am looking for a Ph.D. position starting in 2026 Fall. Please feel free to reach out!</span>


## üìù Publications 
(*: equal contribution; ‚Ä†: corresponding author)


Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, **Yibin Liu**, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu‚Ä†, Zhi Hou‚Ä†. Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning. **arXiv 2025**. ([Paper](https://arxiv.org/pdf/2510.11027) / [Code](https://github.com/OpenGVLab/Vlaser?tab=readme-ov-file) / [Webpage](https://internvl.github.io/blog/2025-10-11-Vlaser/)) <img alt="GitHub repo stars" src="https://img.shields.io/github/stars/OpenGVLab/Vlaser">


Tianxing Chen*, Zanxin Chen*, Baijun Chen*, Zijian Cai*, **Yibin Liu***, ... Ping Luo‚Ä†, Yao Mu‚Ä†. RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation. **arXiv 2025**. ([Paper](https://arxiv.org/pdf/2506.18088) / [Webpage](https://robotwin-platform.github.io) / [Repo](https://github.com/RoboTwin-Platform/RoboTwin)) <img alt="GitHub repo stars" src="https://img.shields.io/github/stars/RoboTwin-Platform/RoboTwin">


Nan Gao‚Ä†, **Yibin Liu**, Xin Tang, Yanyan Liu, Chun Yu, Yun Huang, Yuntao Wang, Flora D. Salim, Xuhai Orson Xu, Jun Wei, Yuanchun Shi. The Homework Wars: Exploring Emotions, Behaviours, and Conflicts in Parent-Child Homework Interactions. **ACM IMWUT/UbiComp 2025**. ([Paper](https://arxiv.org/abs/2502.01325v2))

**Yibin Liu***, Zhixuan Liang*, Zanxing Chen*, Tianxing Chen, Mengkang Hu,
  Wanxi Dong, Congsheng Xu, Zhaoming Han, Yusen Qin, Yao Mu‚Ä†. HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents. **ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic Intelligence**. ([Paper](https://arxiv.org/abs/2508.02629)/ [Code](https://github.com/NEUIR/Self-Guide))

**Yibin Liu**, Zhenghao Liu‚Ä†, Yukun Yan, Shi Yu, Shuo Wang, Liner Yang, Yu Gu, Ge Yu, Huimin Chen. Self-Guide: A LLM Reasoning Enhancement Method Based on Self-Guided Planning. **CCL 2024 / Journal of Chinese Information Processing**. ([Paper EN](https://github.com/10-OASIS-01/10-OASIS-01.github.io/blob/master/assets/_CCL2024__Self_Guide__A_LLM_Reasoning_Enhancement_Method_Based_on_Self_Guided_Planning_EN_-4.pdf) / [Paper CN](https://10-oasis-01.github.io/assets/183_self_guide_.pdf)/ [Code](https://robotwin-platform.github.io/doc/usage/expert-code-gen.html))

## üìñ Research Experiences


### **University of North Carolina at Chapel Hill ‚Äì Research Intern**

**Advisor:** [Prof. Mingyu Ding](https://dingmyu.github.io) ¬∑ üìÖ **June 2025 ‚Äì Present**, Remote

At UNC, I work on enhancing multimodal large language models (MLLMs) with **spatial understanding** and **action-level reasoning** for robotic manipulation, by leveraging **human language instructions** and **interactive guidance in augmented reality (AR) environments**. This research explores how embodied agents can better ground language in physical contexts to perform complex tasks more effectively.

### **Shanghai Jiao Tong University ‚Äì Research Assistant**

**Advisor:** [Prof. Yao Mu](https://yaomarkmu.github.io) ¬∑ üìÖ **March 2025 ‚Äì Present**, Shanghai, China

At SJTU, I serve as an equal first author and core contributor of **RoboTwin 2.0**, a scalable benchmark for robust bimanual robotic manipulation. I led the design and implementation of the **robot policy code generation agent**, providing the foundation for robust policy data generation in embodied agents.


### **Tsinghua University ‚Äì [Pervasive HCI Lab](https://pi.cs.tsinghua.edu.cn/) ‚Äì Research Assistant**

**Advisor:** [A/Prof Nan Gao](https://nancygao.com/), [A/Prof Chun Yu](https://pi.cs.tsinghua.edu.cn/lab/people/ChunYu/) ¬∑ üìÖ **June 2024 ‚Äì January 2025**, Beijing, China

At Tsinghua, I developed **LLM-based methods** to infer **human behaviors and mental states** from dialogue data, aiming to enhance self-awareness and promote well-being. I combined **LLM-driven analysis** with **expert qualitative coding** to process large-scale family education data and built a **recommendation system** that provides personalized guidance for parent‚Äìchild interactions.



## üè¢ Industry Experiences

### **Horizon Robotics ‚Äì Cloud Platform Intern**

**Mentor:** [Yusen Qin (VP of Technology, D-Robotics)](https://www.linkedin.com/in/yusen-qin-5b23345b/?originalSubdomain=cn) ¬∑ üìÖ **June 2025 ‚Äì Present**, Beijing, China ¬∑ Hybrid

At Horizon Robotics, I focus on the development of the **RDK-agent** . I am building an **LLM-powered Copilot system** within VSCode to support robotic system development, including **automatic coding, environment setup, test generation, code explanation, and manipulation data acquisition**, bridging cutting-edge language models with industrial-grade robotics development workflows.



## üí¨ Talks
- 2024.08, ‚ÄúRetrieval-Augmented Generation Modeling‚Äù for Mingtong Weilai (Beijing) Digital Health Science & Technology Research Institute.

## üë• Academic Service

- Co-Founder of [VapourX](https://vapour-x.cn), an open community for embodied AI beginners, enthusiasts, and researchers.

- Student Committee of [TriFusion Workshop @ SIGGRAPH Asia 2025](https://sa2025.siggraph.org/) ‚Äî *Towards Embodied Intelligence Across Humans, Avatars, and Humanoid Robotics* (responsible for workshop email communications).

- Contributor of [Embodied-AI-Guide](https://github.com/TianxingChen/Embodied-AI-Guide) <img alt="GitHub repo stars" src="https://img.shields.io/github/stars/TianxingChen/Embodied-AI-Guide">

- Reviewer for [Chinese CHI 2024](http://chchi.icachi.org/24/).
  
## üèÜ Awards

- 2025.07 Outstanding Poster at ChinaSI 2025 (**Ranking 1st among 61 posters**, RoboTwin 2.0).
- 2024.11 Outstanding Individual in Technological Innovation of Northeastern University.
- 2024.05 Finalist of Mathematical Contest in Modeling (MCM/ICM 2024, **Top 1.69% in 10,387 teams**).
- 2023.10 National Level Third Prize in 2023 RoboCup China Competition Simulation 3D League Simulation (RoboCup 2023).
- 2023.10 National Level Second Prize in 2023 FIRA SimuroSot China Competition (RoboCup 2023).
- 2023.11 Future Technology Taihu Scholarship.
- 2023.09 Excellent Student Scholarship in Northeastern University.


## üíª Projects

### **RDK Copilot: LLM-powered Development Copilot for Robotics at Horizon Robotics**
- Developed and deployed a **VSCode plugin** that assists robotic system development using LLMs. Supported features include automatic coding, environment setup, code completion, test generation, code explanation, and manipulation data acquisition.

### **[MinRL: Minimal, Clean Code for Reinforcement Learning](https://github.com/10-OASIS-01/minrl)**  <img alt="GitHub repo stars" src="https://img.shields.io/github/stars/10-OASIS-01/minrl">
- **Recognized and pinned by [MathFoundationRL](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning)**, the most popular RL course on Chinese platforms, under "Third-party code and materials."
  
### **[Bencao RAG Medical Intelligent Assistant](https://github.com/10-OASIS-01/BenCao_RAG)**
- Developed a medical knowledge question-answering system that integrates context awareness, internet access, knowledge graphs, and RAG method to provide accurate and personalized medical information.



## üõ†Ô∏è Technologies

**Languages:** Python, C++, C, HTML/CSS, JavaScript, Swift, SQL, MATLAB/Simulink, LaTeX

**Technologies:** PyTorch, Hugging Face, scikit-learn, ROS, OpenCV, NumPy, Git, RAG, Linux, SLAM, iOS Development (SwiftUI, UIKit), ARKit, RealityKit


---

*"Live, travel, adventure, bless, and don't be sorry.üåç‚ú®"* ‚Äî Jack Kerouac

<!--
---
permalink: /
title: "Yibin (L√©on) Liu"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

-->



